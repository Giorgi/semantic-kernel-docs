---
title: Prompts
description: Understanding the importance of prompts in directing the behavior of AIs
author: sophialagerkranspandey
ms.topic: prompt-engineering
ms.author: sopand
ms.date: 02/07/2023
ms.service: semantic-kernel
---
# What are prompts?

Prompts play a crucial role in communicating and directing the behavior of Large Language Models (LLMs) AI. They serve as inputs or queries that users can provide to elicit specific responses from a model. 


## The subtleties of prompting
Effective prompt design is essential to achieving desired outcomes with LLM AI models. Prompt engineering, also known as prompt design, is an emerging field that requires creativity and attention to detail. It involves selecting the right words, phrases, symbols, and formats that guide the model in generating high-quality and relevant texts.

If you've already experimented with ChatGPT, you can see how the model's behavior changes dramatically based on the inputs you provide. For example, the following prompts produce very different outputs:

```Prompt
Please give me the history of humans.
```

```Prompt
Please give me the history of humans in 3 sentences.
```

The first prompt produces a long report, while the second prompt produces a concise response. If you were building a UI with limited space, the second prompt would be more suitable for your needs. Further refined behavior can be achieved by adding even more details to the prompt, but its possible to go too far and produce irrelevant outputs. As a prompt engineer, you must find the right balance between specificity and relevance.

When you work directly with LLM models, you can also use other controls to influence the model's behavior. For example, you can use the `temperature` parameter to control the randomness of the model's output.  Other parameters like top-k, top-p, frequency penalty, and presence penalty also influence the model's behavior.

## Prompt engineering: a new career
Because of the amount of control that exists, prompt engineering is a critical skill for anyone working with LLM AI models. It's also a skill that's in high demand as more organizations adopt LLM AI models to automate tasks and improve productivity. A good prompt engineer can help organizations get the most out of their LLM AI models by designing prompts that produce the desired outputs.

### Becoming a great prompt engineer with Semantic Kernel
Semantic Kernel is a valuable tool for prompt engineering because it allows you to experiment with different prompts and parameters across multiple different models using a common interface. This allows you to quickly compare the outputs of different models and parameters, and iterate on prompts to achieve the desired results.

Once you've become familiar with prompt engineering, you can also use Semantic Kernel to apply your skills to real-world scenarios. By combining your prompts with native functions and connectors, you can build powerful AI-powered applications.

Lastly, by deeply integrating with Visual Studio Code, Semantic Kernel also makes it easy for you to integrate prompt engineering into your existing development processes.

> [!div class="checklist"]
> * Create prompts directly in your preferred code editor.
> * Write tests for them using your existing testing frameworks.
> * And deploy them to production using your existing CI/CD pipelines.

### Additional tips for prompt engineering
Becoming a skilled prompt engineer requires a combination of technical knowledge, creativity, and experimentation. Here are some tips to excel in prompt engineering:

- **Understand LLM AI models:** Gain a deep understanding of how LLM AI models work, including their architecture, training processes, and behavior.
- **Domain knowledge:** Acquire domain-specific knowledge to design prompts that align with the desired outputs and tasks.
- **Experimentation:** Explore different parameters and settings to fine-tune prompts and optimize the model's behavior for specific tasks or domains.
- **Feedback and iteration:** Continuously analyze the outputs generated by the model and iterate on prompts based on user feedback to improve their quality and relevance.
- **Stay updated:** Keep up with the latest advancements in prompt engineering techniques, research, and best practices to enhance your skills and stay ahead in the field.

Prompt engineering is a dynamic and evolving field, and skilled prompt engineers play a crucial role in harnessing the capabilities of LLM AI models effectively.


# Prompting AI models with Semantic Kernel

Prompts are core to getting the correct results from AI models. In this article, we'll demonstrate how to use common prompt engineering techniques while using Semantic Kernel.

If you want to see the final solution to this tutorial, you can check out the following samples in the public documentation repository.

| Language  | Link to final solution |
| --- | --- |
| C# | [Open example in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/DocumentationExamples/Prompts.cs) |
| Java | [Open example in GitHub](https://github.com/microsoft/semantic-kernel/blob/java-v1/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java) |
| Python | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/prompts.py) |

## Creating a prompt that detects the intent of a user

If you've ever used ChatGPT or Microsoft Copilot, you're already familiar with prompting. Given a request, an LLM will attempt to predict the most likely response. For example, if you sent the prompt `"I want to go to the "`, an AI service might return back `"beach"` to complete the sentence. This is a very simple example, but it demonstrates the basic idea of how text generation prompts work.

With the Semantic Kernel SDK, you can easily run prompts from your own applications. This allows you to leverage the power of AI models in your own applications.

One common scenario is to detect the intent of a user so  you could run some automation afterwards, so in this article, we'll show how you can create a prompt that detects a user's intent. Additionally, we'll demonstrate how to progressively improve the prompt by using prompt engineering techniques.

> [!Tip]
> Many of the recommendations in this article are based on the [Prompt Engineering Guide](https://www.promptingguide.ai/introduction/basics). If you want to become an expert at writing prompts, we highly recommend reading it and leveraging their prompt engineering techniques.

## Running your first prompt with Semantic Kernel
If we wanted an AI to detect the intent of a user's input, we could simply _ask_ what the intent is. In Semantic Kernel, we could create a string that does just that with the following code:

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="InitialPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="InitialPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="18,21":::

---

To run this prompt, we now need to create a kernel with an AI service.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="KernelCreation":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="KernelCreation":::

# [Python](#tab/python)

1. Import Semantic Kernel.

    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="7":::

2. Create the kernel.

    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="12":::

3. Add the service to the kernel.

---

Finally, we can invoke our prompt using our new kernel.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="InvokeInitialPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="InvokeInitialPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="24-28":::

---

If we run this code with the input "I want to send an email to the marketing team celebrating their recent milestone.", we should get an output that looks like the following:

```console
The intent of this request is to seek guidance or clarification on how to effectively compose an email to the marketing team in order to celebrate their recent milestone.
```

## Improving the prompt with prompt engineering

While this prompt "works", it's not very usable since you cannot use the result to predictably trigger automation. Every time you run the prompt, you may get a very different response.

To make the result more predictable, we can perform the following improvements:

1. [Make the prompt more specific](#1-make-the-prompt-more-specific)
2. [Add structure to the output with formatting](#2-add-structure-to-the-output-with-formatting)
3. [Provide examples with few-shot prompting](#3-provide-examples-with-few-shot-prompting)
4. [Tell the AI what to do to avoid doing something wrong](#4-tell-the-ai-what-to-do-to-avoid-doing-something-wrong)
5. [Provide context to the AI](#5-provide-context-to-the-ai)
6. [Using message roles in chat completion prompts](#6-using-message-roles-in-chat-completion-prompts)
7. [Give your AI words of encouragement](#7-give-your-ai-words-of-encouragement)

### 1) Make the prompt more specific

The first thing we can do is be more specific with our prompt. Instead of just asking "What is the intent of this request?", we can provide the AI with a list of intents to choose from. This will make the prompt more predictable since the AI will only be able to choose from the list of intents we provide.


# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="MoreSpecificPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="MoreSpecificPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="32-33" highlight="2":::

---

Now when you run the prompt with the same input, you should get a more usable result, but it's still not perfect since the AI responds with additional information.

```console
The intent of the request is to send an email. Therefore, the appropriate action would be to use the SendEmail function.
```

### 2) Add structure to the output with formatting

While the result is more predictable, there's a chance that the LLM responds in such a way that you cannot easily parse the result. For example, if the LLM responded with "The intent is SendEmail", you may have a hard time extracting the intent since it's not in a predictable location.

To make the result more predictable, we can add structure to the prompt by using formatting. In this case, we can define the different parts of our prompt like so:

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="StructuredPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="StructuredPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="44-47":::

---

By using this formatting, the AI is less likely to respond with a result that is more than just the intent.

In other prompts, you may also want to experiment with using Markdown, XML, JSON, YAML or other formats to add structure to your prompts and their outputs. Since LLMs have a tendency to generate text that looks like the prompt, it's recommended that you use the same format for both the prompt and the output.

For example, if you wanted the LLM to generate a JSON object, you could use the following prompt:

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="FormattedPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="FormattedPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="58-64":::

---

This would result in the following output:

```json
{
    "intent": "SendEmail"
}
```

### 3) Provide examples with few-shot prompting

So far, we've been using zero-shot prompting, which means we're not providing any examples to the AI. While this is ok for getting started, it's not recommended for more complex scenarios since the AI may not have enough training data to generate the correct result.

To add examples, we can use few-shot prompting. With few-shot prompting, we provide the AI with a few examples of what we want it to do.  For example, we could provide the following examples to help the AI distinguish between sending an email and sending an instant message.


# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="FewShotPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="FewShotPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="91-101" highlight="4-5,7-8":::

---

### 4) Tell the AI what to do to avoid doing something wrong

Often when an AI starts responding incorrectly, it's tempting to simply tell the AI to stop doing something. Unfortunately, this can often lead to the AI doing something even worse. For example, if you told the AI to stop returning back a hallucinated intent, it may start returning back an intent that is completely unrelated to the user's request.

Instead, it's recommended that you tell the AI what it should do _instead_. For example, if you wanted to tell the AI to stop returning back a hallucinated intent, you might write the following prompt.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="AvoidPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="AvoidPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="112-123" highlight="2":::

---

### 5) Provide context to the AI

In some cases, you may want to provide the AI with context so it can better understand the user's request. This is particularly important for long running chat scenarios where the intent of the user may require context from previous messages.

Take for example, the following conversation:
    
```
User: I hate sending emails, no one ever reads them.
AI: I'm sorry to hear that. Messages may be a better way to communicate.
User: I agree, can you send the full status update to the marketing team that way?
```

If the AI was only given the last message, it may incorrectly respond with "SendEmail" instead of "SendMessage". However, if the AI was given the entire conversation, it may be able to understand the intent of the user.

To provide this context, we can simply add the previous messages to the prompt. For example, we could update our prompt to look like the following:

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="ContextPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="ContextPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/prompts.py" range="134-150" highlight="1-4,15":::

---

### 6) Using message roles in chat completion prompts

As your prompts become more complex, you may want to use message roles to help the AI differentiate between system instructions, user input, and AI responses. This is particularly important as we start to add the chat history to the prompt. The AI should know that some of the previous messages were sent by itself and not the user.

In Semantic Kernel, a special syntax is used to define message roles. To define a message role, you simply wrap the message in `<message>` tag with the role name as an attribute. This is currently only available in the C# and Java SDKs.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="RolePrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="RolePrompt":::

# [Python](#tab/python)

<!-- empty for now -->

---

### 7) Give your AI words of encouragement

Finally, research has shown that giving your AI words of encouragement can help it perform better. For example, offering bonuses or rewards for good results can yield better results. 

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Prompts.cs" id="BonusPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Prompts.java" id="BonusPrompt":::

# [Python](#tab/python)

<!-- empty for now -->

---

# Templatizing your prompts

In the previous section, we created a prompt that could be used to get the intent of the user. This function, however, is not very reusable. Because the options are hard coded in. We could dynamically create the prompts string, but there's a better way: prompt templates. function.

By following this example, you'll learn how to templatize a prompt. If you want to see the final solution, you can check out the following samples in the public documentation repository. Use the link to the previous solution if you want to follow along.

| Language  | Link to previous solution | Link to final solution |
| --- | --- |
| C# | [Open example in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/DocumentationExamples/Templates.cs) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/KernelSyntaxExamples/Example31_SerializingPrompts.cs) |
| Java |  | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/java-v1/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java) |
| Python | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/serializing_prompts.py) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/templates.py) |

## Adding variables to the prompt
With Semantic Kernel's templating language, we can add tokens that will be automatically replaced with input parameters. To begin, let's build a super simple prompt that uses the Semantic Kernel template syntax language to include enough information for an agent to respond back to the user.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="40-44":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java" id="create_chat":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/templates.py" range="28-49":::

---

The new prompt uses the `request` and `history` variables so that we can include these values when we run our prompt.
To test our prompt, we can create a chat loop so we can begin talking back-and-forth with our agent.
When we invoke the prompt, we can pass in the `request` and `history` variables as arguments.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="6-7":::

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="35-38,91-100,119-146" highlight="20-21":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java" id="use_chat":::

# [Python](#tab/python)

In the Python template, we just need to provide the value for the `history` variable.

1. Import Semantic Kernel.
    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/templates.py" range="7-10" :::

2. Create the kernel.
    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/templates.py" range="15" :::

3. Add the service to the kernel.

    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/service_configurator.py" range="39-46":::

4. Run the prompt in a chat loop.

    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/templates.py" range="51-77" highlight="19-23":::

---

## Using the Handlebars template engine
In addition to the core template syntax, Semantic Kernel also comes with support for the Handlebars templating language in the C# and Java SDK. To use Handlebars, you'll first want to add the Handlebars package to your project.

# [C#](#tab/Csharp)

```console
dotnet add package Microsoft.SemanticKernel.PromptTemplate.Handlebars --prerelease
```

Then import the Handlebars template engine package.

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="8":::

Afterwards, you can create a new prompt using the `HandlebarsPromptTemplateFactory`. Because Handlebars supports loops, we can use it to loop over elements like examples and chat history. This makes it a great fit for the `getIntent` prompt we created in the [previous article](./your-first-prompt.md).

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="66-90" highlight="10-18":::

We can then create the choice and example objects that will be used by the template. In this example, we can use our prompt to end the conversation once it's over. To do this, we'll just provide two valid intents: `ContinueConversation` and `EndConversation`.

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="46-64":::

Finally, you can run the prompt using the kernel. Add the following code within your main chat loop so the loop can be terminated once the intent is `EndConversation`.

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Templates.cs" range="101-117" highlight="14":::

# [Java](#tab/Java)

Functions can be created from handlebars templates:

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java" id="handlebars_prompt":::

This template requires the following variables:
- `choices` -  A list containing `[ContinueConversation, EndConversation]` that are the possible intents of a users request.
- `fewShotExamples` - A list of examples demonstraiting how the AI should classify a statement.
- `history` - The conversation the AI and user has had so far.
- `request` - The users current request

These can be added to the arguments as:

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java" id="handlebars_add_variables_1":::

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java" id="handlebars_add_variables_2":::

The function can then be invoked as normal:

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/Templates.java" id="handlebars_invoke":::

# [Python](#tab/python)

<!-- empty for now -->

---

# Calling functions within a prompt
In the previous section, we demonstrated how to templatize a prompt to make it more reusable. In this article, we'll show you how to call other functions _within_ a prompt to help break up the prompt into smaller pieces. This helps
keep LLMs focused on a single task, helps avoid hitting token limits, and allows you to add native code directly into your prompt.

If you want to see the final solution, you can check out the following samples in the public documentation repository. Use the link to the previous solution if you want to follow along.

| Language  | Link to previous solution | Link to final solution |
| --- | --- | --- |
| C# | [Open example in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/DocumentationExamples/Templates.cs) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/DocumentationExamples/FunctionsWithinPrompts.cs) |
| Java | [Open solution in GitHub](https://github.com/MicrosoftDocs/semantic-kernel-docs/tree/java-v1/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/FunctionsWithinPrompts.java) |
| Python | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/templates.py) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/functions_within_prompts.py) |


## Calling a nested function
In the [previous example](./templatizing-prompts.md), we created a prompt that chats with the user. This function used the previous conversation history to determine what the agent should say next.

Putting the entire history into a single prompt, however, may result in using too many tokens. To avoid this, we can summarize the conversation history before asking for the intent. To do this, we can leverage the `ConversationSummaryPlugin` that's part of the [core plugins package](../agents/plugins/out-of-the-box-plugins.md).

Below, we show how we can update our original prompt to use the `SummarizeConversation` function in the `ConversationSummaryPlugin` to summarize the conversation history before asking for the intent.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/FunctionsWithinPrompts.cs" id="FunctionFromPrompt":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/FunctionsWithinPrompts.java" id="CreateFunctionFromPrompt":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/functions_within_prompts.py" range="43-65" highlight="1":::

---

## Testing the updated prompt
After adding the nested function, you must ensure that you load the plugin with the required function into the kernel.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/FunctionsWithinPrompts.cs" id="KernelCreation":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/FunctionsWithinPrompts.java" id="CreateKernel":::

# [Python](#tab/python)

1. Import Semantic Kernel.
    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/functions_within_prompts.py" range="7-12" :::

2. Create the kernel.
    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/functions_within_prompts.py" range="17" :::

3. Add the service to the kernel.

    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/service_configurator.py" range="39-46":::

4. Import the plugin and add it to the kernel.
    :::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/functions_within_prompts.py" range="34-37":::

---

Afterwards, we can test the prompt by creating a chat loop that makes the history progressively longer.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/FunctionsWithinPrompts.cs" id="Chat":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/FunctionsWithinPrompts.java" id="ChatLoop":::

# [Python](#tab/python)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/functions_within_prompts.py" range="67-91":::

---


## Calling nested functions in Handlebars
In the previous article, we showed how to use the Handlebars template engine to create the `getIntent` prompt. In this article, we'll show you how to update this prompt with the same nested function.

Similar to the previous example, we can use the `SummarizeConversation` function to summarize the conversation history before asking for the intent. The only difference is that we'll need to use the Handlebars syntax to call the function which requires us to use an `-` between the plugin name and function name instead of a `.`.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/FunctionsWithinPrompts.cs" id="IntentFunction":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/FunctionsWithinPrompts.java" id="IntentFunction":::

# [Python](#tab/python)

<!-- empty for now -->
# Configuring prompts

When creating a prompt, you can adjust parameters that control how the prompt behaves. In Semantic Kernel, these parameters both control how a function is run by an AI model and how it used by function calling and [planners](../agents/planners/index.md).

For example, you could add settings to the chat prompt from the previous article with the following code

# [C#](#tab/Csharp)

In C#, you can define the following properties of a prompt:
- **Name** - the name of the prompt
- **Description** - a description of what the prompt does
- **Template format** - the format of the prompt template (e.g., `semantic-kernel`, `handlebars`)
- **Input variables** - the variables that are used inside of the prompt (e.g., `request`)
- **Execution settings** - the settings for different models that can be used to execute the prompt

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/ConfiguringPrompts.cs" id="FunctionFromPrompt":::

# [Java](#tab/Java)

In Java, you can define the following properties of a prompt:
- **Name** - the name of the prompt
- **Description** - a description of what the prompt does
- **Template format** - the format of the prompt template (e.g., `semantic-kernel`, `handlebars`)
- **Input variables** - the variables that are used inside of the prompt (e.g., `request`)
- **Input variables** - the variables that are used inside of the prompt (e.g., `request`)
- **Output variable** - the type of value that returned by the prompt (e.g., `java.lang.String`)
- **Execution settings** - the settings for different models that can be used to execute the prompt

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/ConfiguringPrompts.java" id="CreateFromPrompt":::

# [Python](#tab/python)

In Python, you can define the following properties of a prompt:
- **Name** - the name of the prompt
- **Description** - a description of what the prompt does
- **Execution settings** - the settings used to execute the prompt (e.g., `max_tokens`, `temperature`)

:::code language="python" source="~/../semantic-kernel-samples/python/samples/documentation_examples/configuring_prompts.py" range="43-67":::

---

## Parameters used by planner
The `description` field `input_variables` array are leveraged by [planners](/semantic-kernel/concepts-sk/planner) to determine how to use a function. The `description` tells planner what the function does, and the `input_variables` tells planner how to populate the input parameters.

Because these parameters impact the behavior of planner, we recommend running tests on the values you provide to ensure they are used by planner correctly.

When writing `description` and `input_variables`, we recommend using the following guidelines:
- The `description` fields should be short and concise so that it does not consume too many tokens when used in planner prompt (but not so short that it is not descriptive enough).
- Consider the `description`s of other functions in the same plugin to ensure that they are sufficiently unique. If they are not, planner may not be able to distinguish between them.
- If you have trouble getting planner to use a function, try adding recommendations or examples for when to use the function.

## Execution settings used by AI models
In addition to providing parameters for planner, the execution settings also allows you to control how a function is run by an AI model. The following table describes the many of the commonly available settings for models:

| Completion Parameter | Type | Required? | Default | Description |
|---|---|---|---|
| `max_tokens` | integer| Optional |16 |	The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens can't exceed the model's context length. Most models have a context length of 2048 tokens (except davinci-codex, which supports 4096).|
| `temperature`	| number	| Optional	| 1	| What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. We generally recommend altering this or `top_p` but not both. |
| `top_p`	| number	| Optional	| 1	| An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. |
| `presence_penalty` | number	| Optional	| 0	| Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. |
| `frequency_penalty` |	number	| Optional	|0 |	Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. |

To learn more about the various parameters available for OpenAI and Azure OpenAI models, visit the [Azure OpenAI reference](/azure/cognitive-services/openai/reference) article.

### Default setting for OpenAI and Azure OpenAI
If you do not provide completion parameters, Semantic Kernel will use the default parameters for the OpenAI API. Learn more about the current defaults by reading the [Azure OpenAI API reference](/azure/cognitive-services/openai/reference) article.

# Saving and sharing prompts

In previous section, we demonstrated how to create and run prompts inline. However, in most cases, you'll want to create your prompts in a separate file so you can easily import them into Semantic Kernel across multiple projects and share them with others.

In this article, we'll demonstrate how to create the files necessary for a prompt so you can easily import them into Semantic Kernel. As an example in this article, we will build on the [previous tutorial](./templatizing-prompts.md) by showing how to serialize the chat prompt. This prompt will be called `chat`.

If you want to see the final solution, you can check out the following samples in the public documentation repository. Use the link to the previous solution if you want to follow along.

| Language  | Link to previous solution | Link to final solution |
| --- | --- |
| C# | [Open example in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/DocumentationExamples/FunctionsWithinPrompts.cs) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/DocumentationExamples/SerializingPrompts.cs) |
| Java | - | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/java-v1/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/SerializingPrompts.java) |
| Python | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/functions_within_prompts.py) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/serializing_prompts.py) |


## Creating a home for your prompts
Before creating the files for the `chat` function, you must first define a folder that will hold all of your plugins. This will make it easier to import them into Semantic Kernel later. We recommend putting this folder at the root of your project and calling it _Prompts_.

Within your _Prompts_ folder, you can create a nested folder called _chat_ for your function.

```directory
Prompts
│
└─── chat
```

## Creating the files for your prompt
Once inside of a prompts folder, you'll need to create two new files: _skprompt.txt_ and _config.json_. The _skprompt.txt_ file contains the prompt that will be sent to the AI service and the _config.json_ file contains the configuration along with semantic descriptions that can be used by planners.

Go ahead and create these two files in the _chat_ folder.

```directory
Prompts
│
└─── chat
     |
     └─── config.json
     └─── skprompt.txt
```

### Writing a prompt in the _skprompt.txt_ file
The _skprompt.txt_ file contains the request that will be sent to the AI service. Since we've already written the prompt in the [previous tutorial](./templatizing-prompts.md), we can simply copy it over to the _skprompt.txt_ file.

:::code language="txt" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Plugins/Prompts/chat/skprompt.txt":::


### Configuring the function in the _config.json_ file
Next, we need to define the configuration for the `chat` function. When serializing the configuration, all you need to do is define the same properties in a JSON file:

- `type` – The type of prompt. In this case, we're using the `completion` type.
- `description` – A description of what the prompt does. This is used by planner to automatically orchestrate plans with the function.
- `completion` – The settings for completion models. For OpenAI models, this includes the `max_tokens` and `temperature` properties.
- `input` – Defines the variables that are used inside of the prompt (e.g., `input`).

For the `chat` function, we can use the same configuration [as before](./configure-prompts.md).

# [C#](#tab/Csharp)

:::code language="json" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Plugins/Prompts/chat/config.json":::

# [Java](#tab/Java)

:::code language="json" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/resources/Plugins/Prompts/chat/config.json":::

# [Python](#tab/python)

:::code language="json" source="~/../samples/python/07-Serializing-Prompts/prompts/chat/config.json":::

---

### Testing your prompt

At this point, you can import and test your function with the kernel by updating your _Program.cs_, _main.py_ or _Main.java_ file to the following.

# [C#](#tab/Csharp)

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/SerializingPrompts.cs" range="8-10,38-46,74-81,99-129":::

# [Java](#tab/Java)

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/SerializingPrompts.java" id="InvokeSerializedPrompts":::

# [Python](#tab/python)

:::code language="python" source="~/../samples/python/07-Serializing-Prompts/main.py" range="2-3,5-10,12-41":::

---

## Using YAML to serialize your prompt

In addition to the _skprompt.txt_ and _config.json_ files, you can also serialize your prompt using a single YAML file while using the C# or Java SDK. This is useful if you want to use a single file to define your prompt. Additionally, this is the same format that is used by Azure AI Studio, making it easier to share prompts between the two platforms.

Let's try creating a YAML serialization file for the `getIntent` prompt.

# [C#](#tab/Csharp)

To get started, you first need to install the necessary packages.

```console
dotnet add package Microsoft.SemanticKernel.Yaml --prerelease
```

This walkthrough also uses the Handlebars template engine, so you'll need to install the Handlebars package as well.

```console
dotnet add package Microsoft.SemanticKernel.PromptTemplate.Handlebars --prerelease
```

Next, create a new file called _getIntent.prompt.yaml_ in the _Prompts_ folder and copy the following YAML into the file.

:::code language="yaml" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/Resources/getIntent.prompt.yaml":::

You should notice that all of the same properties that were defined in the _config.json_ file are now defined in the YAML file. Additionally, the `template` property is used to define the prompt template.

As a best practice, we recommend adding your prompts as an embedded resource. To do this, you'll need to update your _csproj_ file to include the following:

```xml
<ItemGroup>
     <EmbeddedResource Include="Prompts\**\*.yaml" />
</ItemGroup>
```

Finally, you can import your prompt in the _Program.cs_ file.

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/SerializingPrompts.cs" range="47-52":::

To call the prompt, you can use the following code:

:::code language="csharp" source="~/../semantic-kernel-samples/dotnet/samples/LearnResources/MicrosoftLearn/SerializingPrompts.cs" range="82-92":::

# [Java](#tab/Java)

Create a new file called _getIntent.prompt.yaml_ in the _Prompts_ folder and copy the following YAML into the file.

:::code language="yaml" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/resources/Plugins/Prompts/getIntent.prompt.yaml":::

As a best practice, we recommend adding your prompts as an embedded resource.

Finally, you can import your prompt in the Java file.

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/SerializingPrompts.java" id="LoadPromptFromYaml":::

To call the prompt, you can use the following code:

:::code language="java" source="~/../semantic-kernel-samples-java/java/samples/sample-code/src/main/java/com/microsoft/semantickernel/samples/documentationexamples/SerializingPrompts.java" id="InvokePromptFromYaml":::


---
# Prompt template syntax


The Semantic Kernel prompt template language is a simple way to
define and compose AI functions using plain text.
You can use it to create natural language prompts, generate responses, extract
information, invoke other prompts or perform any other task that can be
expressed with text.

The language supports three basic features that allow you to 1) include
variables, 2) call external functions, and 3) pass parameters to functions.

You don't need to write any code or import any external libraries, just use the
curly braces `{{...}}` to embed expressions in your prompts.
Semantic Kernel will parse your template and execute the logic behind it.
This way, you can easily integrate AI into your apps with minimal effort and
maximum flexibility.

> [!TIP]
> If you need more capabilities, we also support the [Handlebars](https://handlebarsjs.com/)
> template engine, which allows you to use loops, conditionals, and other advanced
> features. See how to use the [Handlebars template engine here](./templatizing-prompts.md#using-the-handlebars-template-engine).

## Variables

To include a variable value in your prompt, use the `{{$variableName}}` syntax.
For example, if you have a variable called `name` that holds the user's name,
you can write:

```Hello {{$name}}, welcome to Semantic Kernel!```

This will produce a greeting with the user's name.

Spaces are ignored, so if you find it more readable, you can also write:

```Hello {{ $name }}, welcome to Semantic Kernel!```

## Function calls

To call an external function and embed the result in your prompt, use the
`{{namespace.functionName}}` syntax.
For example, if you have a function called `weather.getForecast` that returns
the weather forecast for a given location, you can write:

```The weather today is {{weather.getForecast}}.```

This will produce a sentence with the weather forecast for the default location
stored in the `input` variable.
The `input` variable is set automatically by the kernel when invoking a function.
For instance, the code above is equivalent to:

```The weather today is {{weather.getForecast $input}}.```

## Function parameters

To call an external function and pass a parameter to it, use the
`{{namespace.functionName $varName}}` and
`{{namespace.functionName "value"}}` syntax.
For example, if you want to pass a different input to the weather forecast
function, you can write:


```
The weather today in {{$city}} is {{weather.getForecast $city}}.
The weather today in Schio is {{weather.getForecast "Schio"}}.
```

This will produce two sentences with the weather forecast for two different
locations, using the city stored in the `city` variable and the _"Schio"_
location value hardcoded in the prompt template.

## Notes about special chars

Semantic function templates are text files, so there is no need to escape special chars
like new lines and tabs. However, there are two cases that require a special syntax:

1. Including double curly braces in the prompt templates
2. Passing to functions hardcoded values that include quotes

## Prompts needing double curly braces

Double curly braces have a special use case, they are used to inject variables,
values, and functions into templates.

If you need to include the **`{{`** and **`}}`** sequences in your prompts, which
could trigger special rendering logic, the best solution is to use string values
enclosed in quotes, like `{{ "{{" }}` and `{{ "}}" }}`

For example:


```{{ "{{" }} and {{ "}}" }} are special SK sequences.```

will render to:

```{{ and }} are special SK sequences.```

## Values that include quotes, and escaping

Values can be enclosed using **single quotes** and **double quotes**.

To avoid the need for special syntax, when working with a value that contains
_single quotes_, we recommend wrapping the value with _double quotes_. Similarly,
when using a value that contains _double quotes_, wrap the value with _single quotes_.

For example:


```
...text... {{ functionName "one 'quoted' word" }} ...text...
...text... {{ functionName 'one "quoted" word' }} ...text...
```

For those cases where the value contains both single and double quotes, you will
need _escaping_, using the special **«`\`»** symbol.

When using double quotes around a value, use **«`\"`»** to include a double quote
symbol inside the value:


```... {{ "quotes' \"escaping\" example" }} ...```

and similarly, when using single quotes, use **«`\'`»** to include a single quote
inside the value:


```... {{ 'quotes\' "escaping" example' }} ...```

Both are rendered to:

```... quotes' "escaping" example ...```

Note that for consistency, the sequences **«`\'`»** and **«`\"`»** do always render
to **«`'`»** and **«`"`»**, even when escaping might not be required.

For instance:


```... {{ 'no need to \"escape" ' }} ...```

is equivalent to:

```... {{ 'no need to "escape" ' }} ...```

and both render to:

```... no need to "escape"  ...```

In case you may need to render a backslash in front of a quote, since **«`\`»**
is a special char, you will need to escape it too, and use the special sequences
**«`\\\'`»** and **«`\\\"`»**.

For example:


```{{ 'two special chars \\\' here' }}```

is rendered to:

```two special chars \' here```

Similarly to single and double quotes, the symbol **«`\`»** doesn't always need
to be escaped. However, for consistency, it can be escaped even when not required.

For instance:


```... {{ 'c:\\documents\\ai' }} ...```

is equivalent to:

```... {{ 'c:\documents\ai' }} ...```

and both are rendered to:

```... c:\documents\ai ...```

Lastly, backslashes have a special meaning only when used in front of
**«`'`»**, **«`"`»** and **«`\`»**.

In all other cases, the backslash character has no impact and is rendered as is.
For example:


```{{ "nothing special about these sequences: \0 \n \t \r \foo" }}```

is rendered to:

```nothing special about these sequences: \0 \n \t \r \foo```


