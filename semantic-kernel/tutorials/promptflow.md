---
title: Why use Prompt flow with Semantic Kernel?
description: Learn how to use Prompt flow to evaluate and deploy your Semantic Kernel applications.
author: matthewbolanos
ms.topic: tutorial
ms.author: mabolan
ms.date: 07/12/2023
ms.service: semantic-kernel
---

# Using Prompt flow with Semantic Kernel


Previously, we demonstrated the importance of providing descriptions for your plugins so planners can effectively use them for autogenerated plans. Knowing whether or not your descriptions are effective, however, can be difficult. In this section, we'll describe how you can use [Prompt flow](/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) to evaluate plugins and planners to ensure that they are consistently producing the desired results.

> [!Note]
> Today Prompt flow is only available in Python, so this section will only show how to use Prompt flow to evaluate plugins using Python.

## What is Prompt flow?
[Prompt flow](/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) is a development tool designed to streamline the creation of LLM applications. It does so by providing tooling that simplifies the process of prototyping, experimenting, iterating, and deploying LLM applications.

Most notably, Prompt flow allows you to author chains of native and prompts and visualize them as a graph. This allows you and the rest of your team to easily create and test AI powered capabilities in both Azure Machine Learning Studio and locally with VS Code.

:::image type="content" source="../../../media/prompt-flow-in-vs-code.png" alt-text="A graph of a Prompt flow":::

Additionally, as part of the Azure Machine Learning Service, Prompt flow makes it easy to deploy real-time endpoints that allows you to integrate your flows into your existing applications _and_ it provides a robust set of tools to perform evaluations and A/B tests for any new flows.

In this section, we'll be focusing on Prompt flow's evaluation and deployment features to evaluate and deploy Semantic Kernel planners. To learn more about Prompt flow, check out the [Prompt flow overview](/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) article.

### Why use Prompt flow with Semantic Kernel?

With Semantic Kernel you can build autonomous AI applications with the aid of plugins and planners. Creating autonomous AI applications, however, can be challenging because you need to ensure your plugins and planners consistently produce the desired results across a wide range of inputs. This is where Prompt flow can help.


:::row:::
   :::column span="2":::
#### 1) Give Prompt flow the power of planners

        Prompt flow is great at defining and running static chains of functions. This works for many AI applications, but it doesn't work for scenarios where you expect an AI application to automatically adapt to new inputs and scenarios. _This is where Semantic Kernel shines._

        By leveraging planners and plugins within Prompt flow, you can author flows that automatically generate plans based on the needs of your users. This let's you avoid manually define every possible scenario yourself.

#### 2) Easily evaluate Semantic Kernel

        With Prompt flow, you can leverage the power of Azure ML to evaluate the accuracy, performance, and error rates of your plugins and planners.
        
        This allows you to quickly iterate on your plugins and planners because you can easily run tests to validate that you haven't accidentally introduced any regressions. If you use Prompt flow in Azure ML, your teammates can also run evaluations on your flows for collaborative development.

   :::column-end:::
   :::column span="1":::
        ![Semantic Kernel running inside of Prompt flow](/../media/semantic-kernel-in-prompt-flow.png)
   :::column-end:::
:::row-end:::


#### 3) Deploy Semantic Kernel to Azure ML

Lastly, Prompt flow's ability to deploy AI applications to Azure Machine Learning means you can use Prompt flow to easily deploy your Semantic Kernel applications with minimal effort.

### Do I _need_ to use Prompt flow with Semantic Kernel?
No! Prompt flow is an optional tool that you can use to help you create, evaluate, and deploy your Semantic Kernel applications. You can still use Semantic Kernel without Prompt flow by deploying it to your own infrastructure, but you will not be able to leverage Prompt flow's evaluation and deployment features.

Similarly, you can use Prompt flow without Semantic Kernel, but you will not have the ability to create dynamic flows or plans that can address unique customer scenarios.


## Next steps
Now that you understand the value of using Prompt flow with Semantic Kernel, let's walk through how you can use Prompt flow to evaluate and deploy your Semantic Kernel applications. The following articles will walk you through the key steps of using Prompt flow with Semantic Kernel by using the math plugin we created in the planner section.

|   | Topic | Description | 
|:-:|---------|-------------|
| 1. | Creating a Prompt flow with Semantic Kernel | Learn how to use the Prompt flow CLI to create a new flow and then add Semantic Kernel to it. |
| 2. | Running batches with Prompt flow | Run the flow you created in the previous article with large amounts of benchmark data to see how it performs with a wide range of user requests. |
| 3. | Evaluating batch results| Once you've run your flow on top of the benchmark data, you can run an evaluation to see how well it performed. This article also describes how you can improve your plugins and personas after reviewing the evaluation results. |
| 4. | Deploying Semantic Kernel with Prompt flow | Finally, learn how to deploy your plugin to Azure so other apps and services can use your planner. |


# Create a Prompt flow with Semantic Kernel 


In the planner section, we demonstrated how you can use the sequential planner to automatically use the math plugin to answer word problems provided by the user. If you began testing your planner with additional inputs, however, you may have noticed that it doesn't always produce the desired results. In this article, we'll show you how you can create a [Prompt flow](/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) that runs your plugins and planners so that you can easily test, evaluate, and deploy them in following articles.

At the end of this article, you'll have a Prompt flow that can answer questions to math problems using Semantic Kernel.

![Semantic Kernel running inside of Prompt flow](../../../media/prompt-flow-end-result.png)

If you want to see the final solution to this article, you can check out the following samples in the public documentation repository. Use the link to the previous solution if you want to follow along.

> [!Note]
> Today Prompt flow is only available in Python, so this article will only show how to use Prompt flow to evaluate plugins using Python.

| Language | Link to previous solution | Link to final solution |
| --- | --- |
| C# |  Not applicable | Not available |
| Java |  Not applicable | Not available |
| Python |  [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/planner.py) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/evaluate_with_prompt_flow.py) |

## Create a new Prompt flow
In this tutorial, we'll be creating a flow that uses the math plugin we created in the pervious tutorial. We'll use the code-first approach to creating a Prompt flow. If you want full documentation on how to use the code-first approach, please refer to [Prompt flow's open source documentation](https://microsoft.github.io/promptflow/index.html).

If you would like to use the Azure portal to create your flow, you can follow the [How to develop an evaluation flow](/azure/machine-learning/prompt-flow/how-to-develop-an-evaluation-flow) tutorial in the Azure documentation.

### Install Prompt flow
1. Install the `promptflow` and `promptflow-tools` by running the following command in your terminal.

    ```bash
    pip install promptflow promptflow-tools
    ```

2. Validate that the installation was successful by running the following command in your terminal.

    ```bash
    # should print promptflow version, e.g. "0.1.0b3"
    pf -v
    ```

### Install the Prompt flow VS Code extension
We also recommend installing the Prompt flow VS Code extension to help you create and test Prompt flows directly from within VS Code.

1. Install the latest stable version of [VS Code](https://code.visualstudio.com/)
2. Install the [VS Code Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
3. Install the [Prompt flow for VS Code extension](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow)

With the VS Code extension, you'll be able to view your Prompt flow in a visual editor, as well as test your Prompt flow directly from within VS Code.

:::image type="content" source="../../../media/prompt-flow-in-vs-code.png" alt-text="A graph of a Prompt flow":::

### Use the CLI to create a new Prompt flow
Prompt flow has three different types of flows: standard, chat, and evaluation. We'll first create a flow that will run the math plugin we created in the previous tutorial using the standard flow type. This will allow us to test the flow and make sure it's working as expected.

To create a standard flow, navigate to the root of the previously completed solution and run the following command in your terminal. If you haven't completed the previous tutorial, you can use the link at the top of this article to download the completed previous solution.

```bash
pf flow init --flow perform_math
```

This will create a new folder with the name of the flow you specified along with some boilerplate code. You can now open this folder in VS Code to view the files that were created. The most important files to note are the following:
- _flow.dag.yaml_ – The definition of the flow; includes inputs/outputs, nodes, tools and variants.
- _.promptflow/flow.tools.json_ – Provides metadata for the nodes found in the flow.dag.yaml file.
- _Source code files (.py, .jinja2)_ – The custom scripts that are referenced by Prompt flow.
- _requirements.txt_ – The Python package dependencies for this flow; used for installing packages into the Azure environment.

### Visualize the flow in VS Code
If you want to see what your flow looks like in a visual editor, you can open the _flow.dag.yaml_ file in VS Code and click the **Visual editor** link in the top left corner of the editor.

:::image type="content" source="../../../media/visualize-flow-link.png" alt-text="A graph of a Prompt flow in the VS Code extension":::

This will open the flow in a visual editor where you can see the nodes and their connections.

## Edit the new Prompt flow
Now that we have a starter flow, we can start customizing it to use the math plugin we created in the previous tutorial. We'll start by deleting the existing nodes and creating a new node that will call the math plugin.

### Delete the existing nodes
1. Navigate to the visual editor
2. Select the delete icon (the trash can) for the *hello_prompt* and *echo_my_prompt* nodes.
3. Delete the _hello.jinja2_ and _hello.py_ files from the flow folder.

### Create a new node
To run our math plugin, we'll need to create a new node that will actually call the plugin using a Semantic Kernel planner. We'll use the Python node type to do this.

1. Navigate to the visual editor
2. Select the **+** icon to create a new node
3. Select the **Python** node type
4. Name your new node *math_planner*
5. Select **New file**.

After completing these steps, you should see a new node in the visual editor with the name *math_planner* along with a new Python file called *math_planner.py* in the flow folder.

### Wiring up the new node
In the visual editor, you'll likely see an error icon. This is because our node doesn't have any inputs or outputs defined. We'll need to define these so that Prompt flow knows how to wire up the nodes.

To do this, follow these steps:
1. Navigate to the visual editor
2. Rename the **output_prompt** output variable to **result**
3. Change the value of the **result** output variable to `${math_planner.output}`
4. Set the value of the **input1** string of the **math_planner** node to `${inputs.text}`

After completing these steps, you should see that the error icon has disappeared and the nodes are now connected.

:::image type="content" source="../../../media/wired-up-flow.png" alt-text="A valid graph of a Prompt flow":::

If you want, you can also edit the flow directly in the _flow.dag.yaml_ file. The following code shows the updated _flow.dag.yaml_ file. If you are running into issues with the visual editor, you can copy and paste this code into your _flow.dag.yaml_ file.

```yaml
id: template_standard_flow
name: Template Standard Flow
environment:
  python_requirements_txt: requirements.txt
inputs:
  text:
    type: string
outputs:
  result:
    type: string
    reference: ${math_planner.output}
nodes:
- name: math_planner
  type: python
  source:
    type: code
    path: math_planner.py
  inputs:
    input1: ${inputs.text}
```

### Calling the math plugin with the planner
Now that we have our nodes wired up, we can add the code to call the math plugin. To do this, we'll need to update the *math_planner.py* file with the code from the planner tutorial. Below is the updated code for the *math_planner.py* file.

When creating a node for Prompt flow, it's important to use the `@tool` decorator to indicate that the function is a tool that can be called by Prompt flow. 

:::code language="python" source="~/../samples/python/12-Evaluate-with-Prompt-Flow/perform_math/math_planner.py"  highlight="16":::

You'll also need to copy and paste your _plugins_ folder from the previous tutorial into the flow folder so that the math plugin is available to the flow. Below is the updated directory structure for the flow.

```directory
12-Evaluate-with-Prompt-Flow
└───perform_math
|   └───.promptflow
|   |   └───flow.tools.json
|   └───plugins
│   |   └───MathPlugin
|   |       └───Math.py
|   └───.gitignore
|   └───data.jsonl
|   └───flow.dag.yaml
|   └───hello.jinja2
|   └───hello.py
|   └───requirements.txt
└───main.py
```

## Performing an ad-hoc test of your Prompt flow
Our flow is now ready to be tested. To do this, complete the following:
1. Navigate to the visual editor. 
2. Set the value of the **text** input variable to `What is 2 plus 3?`
3. Select the **Run all** button (the double play icon) in the top right corner of the visual editor.

Within the terminal, you should see the following output.

```json
{'result': '5.0'}
```

If you want to test your flow using the SDK, you can also write a simple Python script to do this.


:::code language="python" source="~/../samples/python/12-Evaluate-with-Prompt-Flow/main.py" range="1-4, 8-17":::

## Next steps
Now that you have a flow that can run your plugin, you can start to test where it's performing well and where it's not. Testing each case individually can be time consuming, so in the next article, we'll show you how you can run your flow on a large amount of data to see how well it performs with a wide range of user requests.

# Running batches with Prompt flow


If you want to test your flow with multiple inputs, you can use the batch run feature. This allows you to run your flow with a list of inputs from either a csv, tsv, JSON line file. Afterwards, all of the outputs will be saved to another JSON line file. In the [next article](./evaluating-plugins-and-planners-with-prompt-flow.md) you can then use the output file to evaluate your flow.

![Running a batch with Prompt flow](../../../media/using-batch-runs-with-prompt-flow.png)

To get started, you must first create a JSON lines file that contains sample inputs and the correct ground truth. The following sections will walk you through how to do this.

This article uses the completed solution from the [preview article](./create-a-prompt-flow-with-semantic-kernel.md). If you want to follow along, you can use the following samples in the public documentation repository.

> [!Note]
> Today Prompt flow is only available in Python, so this article will only show how to use Prompt flow to evaluate plugins using Python.

| Language | Link to completed solution |
| --- | --- |
| C# | Not available |
| Java | Not available |
| Python | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/evaluate_with_prompt_flow.py) |

## Create benchmark data for your Prompt flow
All benchmark data can be found in the _data.jsonl_ file. This file contains a list of JSON objects that contains the input and the correct ground truth. Let's update the _data.jsonl_ file with data that we can use to evaluate our plugin.

```json
{"text": "How many sheep would you have if you started with 3 and got 2 more?", "groundtruth": "5"}
{"text": "What would be the area of a rectangle with a sides of 2ft and 3ft?", "groundtruth": "6"}
{"text": "What would you have left if you spent $3 when you only had $2 to begin with", "groundtruth": "-1"}
{"text": "How many slices of pizza would everyone get if you split 12 slices equally among 3 people", "groundtruth": "4"}
{"text": "What is the sum of 5 and 3?", "groundtruth": "8"}
{"text": "Subtract 7 from 10.", "groundtruth": "3"}
{"text": "Multiply 6 by 4.", "groundtruth": "24"}
{"text": "Divide 20 by 5.", "groundtruth": "4"}
{"text": "What is the square of 7?", "groundtruth": "49"}
{"text": "What is the square root of 81?", "groundtruth": "9"}
```

### Run your Prompt flow with the benchmark data
Now that we have our benchmark data, we can run our flow over the data to see how well it performs. There are several ways to run a flow using either the VS Code extension or the CLI.

#### Use the visual editor to run a batch of inputs
1. Open the visual editor.
2. Select the **Batch run** icon (the beaker icon).

    :::image type="content" source="../../../media/prompt-flow-batch-run-button.png" alt-text="Location of the batch run icon":::

3. Select **Local JSON Lines File**.
4. Select the _data.jsonl_ file in the file picker.
5. Select the **Run** button in the new file.

    :::image type="content" source="../../../media/run-link-for-prompt-flow.png" alt-text="Location of the run button for Prompt flow":::

#### Use the CLI to run a batch of inputs
1. Navigate to the root of the flow folder.

    ```bash
    cd ./perform_math
    ```

2. Run the following command in your terminal; we'll use the `--name` parameter to name the run _perform_math_.

    ```bash
    pf run create  --flow . --data data.jsonl --stream --name perform_math
    ```

    > [!Important]
    > The name of evaluations must be unique. If you run the same evaluation twice, you will need to use a different name otherwise the second run will fail.
    

## Viewing the results
If you use the CLI to name your batch run with the `--name` parameter, you can use the following commands to get the results afterwards.

```bash
pf run show-details -n perform_math
pf run visualize -n perform_math
```

After running the `pf run show-details` command, you should see the following output.

# [GPT-3.5-turbo](#tab/gpt-35-turbo)
If you are running the planner with GPT-3.5-turbo, you'll likely run into a few errors, so only _some_ of the results will come back (notice that line 3 failed) and a few of the results may be incorrect (e.g., line 1, 2, and 4).

```output
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|    | inputs.text                                                                               |   inputs.line_number | outputs.result    |
+====+===========================================================================================+======================+===================+
|  0 | How many sheep would you have if you started with 3 and got 2 more?                       |                    0 | 5.0               |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  1 | What would be the area of a rectangle with a sides of 2ft and 3ft?                        |                    1 | 2.449489742783178 |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  2 | What would you have left if you spent $3 when you only had $2 to begin with               |                    2 |                   |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  3 | How many slices of pizza would everyone get if you split 12 slices equally among 3 people |                    3 | (Failed)          |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  4 | What is the sum of 5 and 3?                                                               |                    4 | 5.0               |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  5 | Subtract 7 from 10.                                                                       |                    5 | 3.0               |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  6 | Multiply 6 by 4.                                                                          |                    6 | 24.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  7 | Divide 20 by 5.                                                                           |                    7 | 4.0               |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  8 | What is the square of 7?                                                                  |                    8 | 49.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
|  9 | What is the square root of 81?                                                            |                    9 | 9.0               |
+----+-------------------------------------------------------------------------------------------+----------------------+-------------------+
```

# [GPT-4](#tab/gpt-4)
If you are running the planner with GPT-4, there should be minimal errors, but some of the results may still be incorrect. For example, row 2 should be _negative_ 1 not positive 1.

```output
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|    | inputs.text                                                                               |   inputs.line_number | outputs.result   |
+====+===========================================================================================+======================+==================+
|  0 | How many sheep would you have if you started with 3 and got 2 more?                       |                    0 | 5.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  1 | What would be the area of a rectangle with a sides of 2ft and 3ft?                        |                    1 | 8.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  2 | What would you have left if you spent $3 when you only had $2 to begin with               |                    2 | 1.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  3 | How many slices of pizza would everyone get if you split 12 slices equally among 3 people |                    3 | 4.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  4 | What is the sum of 5 and 3?                                                               |                    4 | 8.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  5 | Subtract 7 from 10.                                                                       |                    5 | 3.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  6 | Multiply 6 by 4.                                                                          |                    6 | 24.0             |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  7 | Divide 20 by 5.                                                                           |                    7 | 4.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  8 | What is the square of 7?                                                                  |                    8 | 49.0             |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
|  9 | What is the square root of 81?                                                            |                    9 | 9.0              |
+----+-------------------------------------------------------------------------------------------+----------------------+------------------+
```

---

As you can see, the results are not yet perfect. In the [next article](./evaluating-plugins-and-planners-with-prompt-flow.md), we'll use Prompt flow's evaluation feature to quantify how well our flow is performing and then we'll update our plugin and planner to improve the results.

### Using the VS Code extension to view the results

If you don't know the name of your run, you can also use the Prompt flow VS Code extension to see a history of all your previous runs and visualize them. To do this, follow these steps:
1. Select the Prompt flow icon in the app bar in VS Code.
2. In the **Batch run history** section, select the refresh button.
3. Select the run you want to view.
4. Select **Visualize & analyze**.
    
    :::image type="content" source="../../../media/prompt-flow-batch-run-history.png" alt-text="Location of the batch run history":::

5. Afterwards, you'll get the same visualization that you'd see if you had run `pf run visualize -n perform_math` in your terminal.

    :::image type="content" source="../../../media/prompt-flow-visualize-batch-run.png" alt-text="The visual of the run":::


### View the logs
To see what the flow is doing, you can open and view the logs of the run. To do this, follow these steps.

1. Run the following command to view the details of the run.

    ```bash
    pf run stream -n perform_math
    ```

2. Before any of the errors are output to the terminal, you should see a run summary

    ```output
    ======= Run Summary =======

    Run name: "perform_math"
    Run status: "Completed"
    Start time: "2023-09-07 11:22:05.160936"
    Duration: "0:00:13.736032"
    Output path: "/Users/<user>/.promptflow/.runs/perform_math"
    ```

3. Copy the value of the `Output path` property.
4. Navigate to the output path. You should see a folder that looks like the following.

    :::image type="content" source="../../../media/prompt-flow-output-folder.png" alt-text="View of the flow artifacts folder":::

5. Open the *node_artifacts/math_planner* folder.
6. Open the one of the JSON line files. These files contain the logs of a single run of your custom node so you can see what your planner is doing. You should see results like the following.

    ```json
    {
        "NodeName": "math_planner",
        "line_number": 3,
        "run_info": {
            "node": "math_planner",
            "flow_run_id": "perform_math",
            "run_id": "perform_math_math_planner_3",
            "status": "Completed",
            "inputs": {
            "input1": "How many slices of pizza would everyone get if you split 12 slices equally among 3 people"
            },
            "output": "4.0",
            "metrics": null,
            "error": null,
            "parent_run_id": "perform_math_3",
            "start_time": "2023-09-05T14:40:55.159904Z",
            "end_time": "2023-09-05T14:41:02.668920Z",
            "index": 3,
            "api_calls": [
            {
                "name": "my_python_tool",
                "type": "Tool",
                "inputs": {
                "input1": "How many slices of pizza would everyone get if you split 12 slices equally among 3 people"
                },
                "output": "4.0",
                "start_time": 1693921255.159942,
                "end_time": 1693921262.668671,
                "error": null,
                "children": null,
                "node_name": "math_planner"
            }
            ],
            "variant_id": "",
            "cached_run_id": null,
            "cached_flow_run_id": null,
            "logs": {
            "stdout": "[2023-09-05T14:41:02+0000] Function: MathPlugin.Divide\n[2023-09-05T14:41:02+0000] Input vars: {'input': '12', 'denominator': '3'}\n[2023-09-05T14:41:02+0000] Output vars: ['RESULT__SLICES_PER_PERSON']\n[2023-09-05T14:41:02+0000] Result: 4.0\n",
            "stderr": ""
            },
            "system_metrics": {
            "duration": 7.509016
            },
            "result": "4.0"
        },
        "start_time": "2023-09-05T14:40:55.159904",
        "end_time": "2023-09-05T14:41:02.668920",
        "status": "Completed"
        }
    ```
    
    Any print statements in your code will be logged in the `run_info.logs.stdout` property. 

## Next steps
Now that you know how to run a batch of inputs on your flow, you can now use the [evaluation feature](./evaluating-plugins-and-planners-with-prompt-flow.md) to quantify the actual performance of your flow.

# Evaluate your plugins and planners with Prompt flow


With the minimal amount of data we provided, we can easily view the results and see that there are some errors. If you have a large amount of data, however, it can be difficult to evaluate the results. This is where Prompt flow's evaluation feature comes in handy.

By the end of this article, you'll learn how to run an evaluation flow to quantify how well your plugins and planners are performing so that you can iterate on your descriptions and hints to improve your results.

![Prompt flow evaluating a plugin](../../../media/evaluating-batch-run-with-prompt-flow.png)

This article uses the completed solution from the [preview article](./create-a-prompt-flow-with-semantic-kernel.md). If you want to follow along, you can use the following samples in the public documentation repository. You will also need an initial batch run to evaluate. If you haven't already, you can follow the steps in the [previous article](./running-batches-with-prompt-flow.md) to create a batch run.

> [!Note]
> Today Prompt flow is only available in Python, so this article will only show how to use Prompt flow to evaluate plugins using Python.

| Language | Link to previous solution | Link to completed solution |
| --- | --- |
| C# | N/A |  Not available |
| Java | N/A | Not available |
| Python | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/evaluate_with_prompt_flow.py) | [Open solution in GitHub](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/documentation_examples/improved_evaluate_with_prompt_flow.py) |


## Using an evaluation flow
In this guide, we'll use one of the existing sample evaluation flows in the [Prompt flow GitHub repo](https://github.com/microsoft/promptflow). With the [eval-accuracy-maths-to-code](https://github.com/microsoft/promptflow/tree/main/examples/flows/evaluation/eval-accuracy-maths-to-code) we can easily evaluate our results and see how well our flow is performing.

1. Download the [eval-accuracy-maths-to-code](https://github.com/microsoft/promptflow/tree/main/examples/flows/evaluation/eval-accuracy-maths-to-code) flow from the [Prompt flow GitHub repo](https://github.com/microsoft/promptflow). 
2. Copy the path to the _eval-accuracy-maths-to-code_ folder.
3. Navigate to the root of the flow folder.

    ```bash
    cd ./perform_math
    ```
3. Run the following command in your terminal after replacing `<path-to-evaluation-flow>` with the path to the _eval-accuracy-maths-to-code_ folder.

    > [!Note]
    > This command assumes the previous batch run was named `perform_math`. If you named your batch run something else, you will need to update the `--run` parameter.

    ```bast
    pf run create --flow <path-to-evaluation-flow> --data ./data.jsonl --column-mapping groundtruth='${data.groundtruth}' prediction='${run.outputs.result}' --run perform_math --stream --name perform_math_eval
    ```

    This command will take the original benchmark data and the results from the batch run and create a new run that contains the evaluation results. The `--column-mapping` parameter tells Prompt flow which columns to use for the ground truth and the prediction. By using the `--name` parameter, we can reference it later when we want to view the results.

4. Run the following command to view the metrics of the evaluation.

    ```bash
    pf run show-metrics -n perform_math_eval
    ```

    You should see results similar to the following:

    # [GPT-3.5-turbo](#tab/gpt-35-turbo)
    With GPT-3.5-turbo, you should expect to see a low accuracy and a high error rate.

    ```json
    {
      "accuracy": 0.6,
      "error_rate": 0.2
    }
    ```

    # [GPT-4](#tab/gpt-4)
    If you are running the planner with GPT-4 the accuracy should be much higher, but it's still not perfect.

    ```json
    {
      "accuracy": 0.9,
      "error_rate": 0.0
    }
    ```

    ---

## Improving your flow with prompt engineering
Once you have a baseline for how well your flow is performing, you can start to improve your descriptions. If you run the following command, you can see which inputs are causing the issues.

```bash
pf run show-details -n perform_math_eval
```

Below is a sample of the results you should see. With this view it's easier to which benchmarks are failing.

# [GPT-3.5-turbo](#tab/gpt-35-turbo)

```output
+----+----------------------+---------------------+----------------------+-----------------+
|    |   inputs.groundtruth | inputs.prediction   |   inputs.line_number |   outputs.score |
+====+======================+=====================+======================+=================+
|  0 |                    5 | 5.0                 |                    0 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  1 |                    6 | 2.449489742783178   |                    1 |               0 |
+----+----------------------+---------------------+----------------------+-----------------+
|  2 |                   -1 |                     |                    2 |              -1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  3 |                    4 | (Failed)            |                    3 |              -1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  4 |                    8 | 5.000000000000007   |                    4 |               0 |
+----+----------------------+---------------------+----------------------+-----------------+
|  5 |                    3 | 3.0                 |                    5 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  6 |                   24 | 24.0                |                    6 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  7 |                    4 | 4.0                 |                    7 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  8 |                   49 | 49.0                |                    8 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  9 |                    9 | 9.0                 |                    9 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
```

# [GPT-4](#tab/gpt-4)

```json
+----+----------------------+---------------------+----------------------+-----------------+
|    |   inputs.groundtruth | inputs.prediction   |   inputs.line_number |   outputs.score |
+====+======================+=====================+======================+=================+
|  0 |                    5 | 5.0                 |                    0 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  1 |                    6 | 8.0                 |                    1 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  2 |                   -1 | 1.0                 |                    2 |              -1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  3 |                    4 | 4.0                 |                    3 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  4 |                    8 | 8.0                 |                    4 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  5 |                    3 | 3.0                 |                    5 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  6 |                   24 | 24.0                |                    6 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  7 |                    4 | 4.0                 |                    7 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  8 |                   49 | 49.0                |                    8 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
|  9 |                    9 | 9.0                 |                    9 |               1 |
+----+----------------------+---------------------+----------------------+-----------------+
```

---

Both GPT-3.5-Turbo and GPT-4 have challenges with the third benchmark `What would you have left if you spent $3 when you only had $2 to begin with?`. This particular benchmark is challenging because it's asking to subtract a larger number from a smaller number to get a negative value. This is something that the planner doesn't know how to easily do.

### Improving the descriptions of your plugin
We can fix the issue by improving the description of the `subtract` function and its input parameters in the math plugin. Below we have updated the description of the `subtract` function to let the planner know how it can handle negative numbers. We've also updated the inputs to reference the mathematical names of the properties (i.e., minuend and subtrahend).

:::code language="python" source="~/../samples/python/13-Improved-Evaluate-with-Prompt-Flow/perform_math/plugins/MathPlugin/math.py" range="33-46" highlight="2,7,11":::

In the final solution, we've also improved the descriptions of some of the other functions to make them more clear to the planner. Check out the math plugin in the final solution and try to see if you can find all the improvements.

### Giving the planner more help
Additionally, we can provide the planner with more guidance on achieving the desired result by adding hints to the original ask. To do so, update the value of the `ask` in the _math_planner.py_ file to include the following hints.

:::code language="python" source="~/../samples/python/13-Improved-Evaluate-with-Prompt-Flow/perform_math/math_planner.py" range="25-28" highlight="3-4":::

## Re-evaluate your flow
Now that we've updated our descriptions and hints, we can re-evaluate our flow to see if it's improved.

1. Navigate to the root of the flow folder.

    ```bash
    cd ./perform_math
    ```

2. Run the following command to re-run the batch.

    ```bash
    pf run create --flow . --data ./data.jsonl --stream --name perform_math_v2
    ```
3. Run the following command to evaluate the new results. Remember to replace `<path-to-evaluation-flow>` with the path to the _eval-accuracy-maths-to-code_ folder.

    ```bash
    pf run create --flow <path-to-evaluation-flow> --data ./data.jsonl --column-mapping groundtruth='${data.groundtruth}' prediction='${run.outputs.result}' --run perform_math_v2 --stream --name perform_math_eval_v2
    ```

4. Run the following command to see the metrics of the evaluation.

    ```bash
    pf run show-metrics -n perform_math_eval_v2
    ```

    You should now see even better results than before.

    # [GPT-3.5-turbo](#tab/gpt-35-turbo)
    While not perfect, GPT-3.5-turbo should now have a higher accuracy and much lower error rate.

    ```json
    {
      "accuracy": 0.8,
      "error_rate": 0.0
    }
    ```

    # [GPT-4](#tab/gpt-4)
    If you are running the planner with GPT-4 the accuracy should now be 1.

    ```json
    {
      "accuracy": 1.0,
      "error_rate": 0.0
    }
    ```

    ---

### Comparing results in VS Code
You can also use the Prompt flow VS Code extension to compare and contrast multiple batch runs. To do this, complete the following:

1. Select the Prompt flow icon in the app bar in VS Code.
2. In the **Batch run history** section, select the refresh button.
3. Select the two runs you want to compare.
4. Select **Visualize & analyze**.

    :::image type="content" source="../../../media/prompt-flow-batch-run-history-compare.png" alt-text="Selecting multiple run histories in the Prompt flow VS Code extension":::

5. Afterwards, you should see a new tab in VS Code that shows the results of the two batch runs side by side.

    :::image type="content" source="../../../media/prompt-flow-batch-run-history-compare-results.png" alt-text="Comparing two batch runs in the Prompt flow VS Code extension":::

## Next steps
Now that you've learned how to evaluate your plugins and planners, you can start to iterate on your descriptions and hints to improve your results. You can also learn how to [deploy your Prompt flow](./deploying-prompt-flows-with-semantic-kernel.md) to an endpoint.

> [!div class="nextstepaction"]
> [Deploy your Prompt flow](./deploying-prompt-flows-with-semantic-kernel.md)

# Deploy Prompt flows with Semantic Kernel to Azure AI Studio


Once you've created a Prompt flow, you can deploy it to Azure ML. This has several benefits:
> [!div class="checklist"]
> * Access to an endpoint that can be called from anywhere
> * Active evaluations of your Prompt flow while running in production
> * Ability to share your Prompt flow with others (including non-developers)

If you would like to deploy your Prompt flow to another service, you can refer to the [open source deployment guide](https://microsoft.github.io/promptflow/how-to-guides/deploy-a-flow/index.html) for Prompt flow.

## Prerequisites
Before beginning, you must first have an Azure AI studio project. If you do not have one, you can create one by following the [Azure AI studio guide](/azure/ai-studio/how-to/create-projects).

## Adding your Prompt flow to Azure AI Studio
Adding your Prompt flow to Azure AI Studio is a simple process. You can either upload your Prompt flow to your workspace or you can create a new Prompt flow and copy and paste your Prompt flow's JSON into the editor. Afterwards, your Prompt flow will be editable in both Azure AI Studio and VS Code for Web. You can even version your Prompt flow using Git with the VS Code for Web integration.

In the following sections, we will walk through the process of adding your Prompt flow to Azure AI Studio using the upload method.

To learn more about Prompt flow in Azure AI Studio, refer to the [Prompt flow in Azure AI Studio guide](/azure/ai-studio/how-to/prompt-flow).

### Upload your Prompt flow to your workspace
To add your Prompt flow to Azure AI Studio, complete the following steps:
1. Open your AI Studio project.
2. Select the **Prompt flow** navigation item on the left.
3. Select the **Create** button to add a new Prompt flow.
    :::image type="content" source="../../../media/create-flow-from-ai-studio.png" alt-text="Create Prompt flow":::

4. Scroll to the bottom of the new Prompt flow panel and select **Import from local**.
5. Select the folder containing your Prompt flow by selecting the **Browse** button.
6. Give your Prompt flow a name and configure it's type.
7. Select **Import**.
    :::image type="content" source="../../../media/import-flow-into-ai-studio.png" alt-text="Import Prompt flow":::

### Update any broken connections
If you have any broken connections in your Prompt flow, you will need to update them. To do so, complete the following steps:
1. Scroll to any nodes with broken connections (e.g., the `math_planner`` node)
2. Select the broken connection and swap it with your preferred connection with the dropdown.
    :::image type="content" source="../../../media/fix-broken-connector-in-ai-studio.png" alt-text="Update broken connections":::

## Running your Prompt flow
Now that you've added your Prompt flow to Azure AI Studio, you can start running it in the cloud. In the following sections, we'll walk through the process of creating a runtime, testing your Prompt flow, and deploying it to an endpoint.

### Create a runtime
If you don't already have a runtime for your Prompt flow, complete the following steps:
1. Select the **+** button next to the **Runtime** dropdown.
2. Give your runtime a name.
3. Select your compute instance; if you don't have one, you can create one by selecting the **Create Azure ML compute instance** link.
4. Select **Use default environment**. The default environment comes pre-installed with the Semantic Kernel package.
5. Select **Create**.
    :::image type="content" source="../../../media/create-runtime-in-ai-studio.png" alt-text="Create runtime":::

### Test your Prompt flow
Once you've created a runtime, you can test your Prompt flow. To do so, complete the following steps:
1. Select your newly created runtime from the **Runtime** dropdown.
2. Select the **Run** button.
    :::image type="content" source="../../../media/run-from-ai-studio.png" alt-text="Run Prompt flow":::
3. Select **View outputs** to view the outputs of your Prompt flow.

Once your flow is in AI Studio, you can also [run batch runs](/azure/ai-studio/how-to/flow-bulk-test-evaluation) and [evaluations](/azure/ai-studio/how-to/flow-develop-evaluation).

### Deploy your Prompt flow
1. Select the **Deploy** button.
2. Define your basic settings.
3. Select **Review + Create**.
4. Review your deployment and select **Create**.

You can now access your Prompt flow's deployment endpoint by selecting the **Deployments** navigation item on the left and selecting your deployment.

To use your deployment, navigate to the **Consume** tab. From there, you can get sample code in a variety of languages to call your Prompt flow's endpoint.

To learn more about deploying Prompt flows refer to the [Deploy a flow for real-time inference](/azure/ai-studio/how-to/flow-deploy?tabs=azure-studio) article.


